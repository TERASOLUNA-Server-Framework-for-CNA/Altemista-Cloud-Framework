
:fragment:

If monitoring or central log system is wanted, it is neccesary to add the Servers and configure each microservice for using them

=== Infrastructure

Create new Infraestructe servers, like the Registry server, and add the features needed, one the Monitoring Server an anohter the zipkin Server

.Zipkin Server
image::altemista-cloudfwk-documentation/microservices/demo/11.Zipkin.png[align="center"]

.Monitoring Server
image::altemista-cloudfwk-documentation/microservices/demo/12.MonitoringServer.png[align="center"]

=== Sleuth

For every micro service add the "Sleuth Feature"

.Monitoring Server
image::altemista-cloudfwk-documentation/microservices/demo/10.Sleuth.png[align="center"]

Also is necessary to configure the Sleuth URL for ALL the microservices, but as we left a file on the config server, update the configserver to distribute that property easy.
.{Config env project}/src/main/resources/config/localconfig/application.properties
[source,properties]
----
...

# Zipkin
spring.zipkin.baseUrl = http://localhost:9411/
----

Once all the services are online, send some request (for instance, queryHealth or QueryAll). After several request we can observe the traces and dependencies on zipkin console (by default not all traces are sent, see the Sleuth configuration on the reference documentation of Altemista Cloud Framework)

.Zipkin Traces
image::altemista-cloudfwk-documentation/microservices/demo/13.ZipkinTraces.png[align="center"]

.Zipkin dependencies
image::altemista-cloudfwk-documentation/microservices/demo/14.ZipkinDependecy.png[align="center"]

=== ELK

Also is possible to use ELK as centralized logger, achaiving that is as easy as include a new appender on the logback configuration.

.[project-env]/src/main/resources/logback/logback.xml
[source,xml]
----
<appender name="STASH" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
	<destination>localhost:5000</destination><!--1-->
	<encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
		<providers>
			<mdc />
			<context />
			<logLevel />
			<loggerName />
			<pattern>
				<pattern>
          {
          "serviceName": "account-service"
          }
        </pattern>
			</pattern>
			<threadName />
			<message />
			<logstashMarkers />
			<stackTrace />
		</providers>
	</encoder>
</appender>
<!--2-->
<logger name="org.terasoluna.gfw.web.logging.ElkLoggingInterceptor">
	<level value="trace" />
	<appender-ref ref="STASH" />
</logger>
----
<1> Configure for your Current ELK instalation
<2> Also create a logger, we will use this logger only for the information that we want centralized

On the service add the logger an call it.

.Controller
[source,java,linenums]
----
//...
	private static Logger elk = LoggerFactory.getLogger("org.terasoluna.gfw.web.logging.ElkLoggingInterceptor");
//...
elk.info("init queryHealth: "+request.toString());
//...
elk.info("end queryHealth: "+response.toString());
//...
----

Also include the dependency on the pom.xml

.[project-env]/pom.xml

[source,xml]
----

<dependency>
	<groupId>net.logstash.logback</groupId>
	<artifactId>logstash-logback-encoder</artifactId>
	<version>4.9</version>
</dependency>
----

[TIP]
====
You can run a ELK with docker compose:

[source,yml]
----
version: '2'
services:
  elasticsearch:
    image: elasticsearch
    ports:
      - '9200:9200'
      - '9300:9300'
  kibana:
    image: kibana
    ports:
      - '5601:5601'
    links:
      - elasticsearch
  logstash:
    image: logstash 
    ports:
      - '5000:5000'
    links:
      - elasticsearch
    command: -e 'input { tcp { port => 5000 codec => "json" } } output { elasticsearch { hosts => ["elasticsearch"] index => "micro-%{serviceName}"} }'
----
====

After some invocations access the kibana and search by the index.

.Kibana with logs
image::altemista-cloudfwk-documentation/microservices/demo/15.ELK.png[align="center"]